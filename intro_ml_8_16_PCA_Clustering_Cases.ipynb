{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STUDY CASES OF PCA AND CLUSTERING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GOAL\n",
    "\n",
    "- Kita ingin clusterkan data client bank\n",
    "- Harapannya dapat **memahami karakter client bank** yang ada terhadap **campaign** yang dilakukan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "# Dataset Information\n",
    "r\n",
    "- The data is related with direct marketing campaigns of a Portuguese banking institution. \n",
    "- The marketing campaigns were based on phone calls. Often, more than one contact to the same client was required, on order to access if the product (bank term deposit) would be (or not) subscribed. \n",
    "- Dataset `bank.csv` ordered by date (from May 2008 to November 2010). \n",
    "- The **exercise goal** is to discover interesting things about the measurement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Variables**\n",
    "​\n",
    "\n",
    "<u>Numeric</u>\n",
    "- `age`\n",
    "- `balance`: average yearly balance, in euros\n",
    "- `duration`: last coontact duration, in seconds\n",
    "- `campaign`: number of contacts performed during this campaign and for this client (numeric, includes last contact)\n",
    "- `pdays`: number of days that passed by after the client was last contacted from a previous campaign (numeric, -1 means client was not previously contacted)\n",
    "- `previous`: number of contacts performed before this campaign and for this client\n",
    "​\n",
    "<u>Categoric</u>\n",
    "- `job` : type of job (categorical) \n",
    "- `marital` : marital status (categorical)\n",
    "- `education` (categorical)\n",
    "- `default`: has credit in default? (binary: \"yes\",\"no\")\n",
    "- `housing`: has housing loan? (binary: \"yes\",\"no\")\n",
    "- `loan`: has personal loan? (binary: \"yes\",\"no\")\n",
    "- `contact`: contact communication type (categorical) \n",
    "- `day`: last contact day of the month \n",
    "- `month`: last contact month of year (categorical)\n",
    "- `poutcome`: outcome of the previous marketing campaign (categorical)\n",
    "\n",
    "\n",
    "Source :  S. Moro, R. Laureano and P. Cortez. Using Data Mining for Bank Direct Marketing: An Application of the CRISP-DM Methodology. <br>\n",
    "  In P. Novais et al. (Eds.), Proceedings of the European Simulation and Modelling Conference - ESM'2011, pp. 117-121, Guimarães, Portugal, October, 2011. EUROSIS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def importData(path, col_to_drop):\n",
    "    # Read Data\n",
    "    data = pd.read_csv(path)\n",
    "    print(f\"Data awal                  : {data.shape}, (#observasi, #fitur)\")\n",
    "\n",
    "    # Drop kolom\n",
    "    data = data.drop(columns = col_to_drop)\n",
    "    print(f\"Data setelah drop kolom    : {data.shape}, (#observasi, #fitur)\")\n",
    "\n",
    "    # Drop duplikat\n",
    "    print(f\"Ada {data.duplicated().sum()} data duplikat\")\n",
    "    data = data.drop_duplicates()\n",
    "    print(f\"Data setelah drop duplikat : {data.shape}, (#observasi, #fitur)\")\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'bank.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m filepath \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbank.csv\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m      2\u001b[0m col_to_drop \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mUnnamed: 0\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m----> 4\u001b[0m data \u001b[39m=\u001b[39m importData(path \u001b[39m=\u001b[39;49m filepath,\n\u001b[0;32m      5\u001b[0m                   col_to_drop \u001b[39m=\u001b[39;49m col_to_drop)\n",
      "Cell \u001b[1;32mIn[2], line 3\u001b[0m, in \u001b[0;36mimportData\u001b[1;34m(path, col_to_drop)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mimportData\u001b[39m(path, col_to_drop):\n\u001b[0;32m      2\u001b[0m     \u001b[39m# Read Data\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m     data \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(path)\n\u001b[0;32m      4\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mData awal                  : \u001b[39m\u001b[39m{\u001b[39;00mdata\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m, (#observasi, #fitur)\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m     \u001b[39m# Drop kolom\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Opal\\anaconda3\\envs\\dashboard_ml_env\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    899\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    900\u001b[0m     dialect,\n\u001b[0;32m    901\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    908\u001b[0m     dtype_backend\u001b[39m=\u001b[39mdtype_backend,\n\u001b[0;32m    909\u001b[0m )\n\u001b[0;32m    910\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 912\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mc:\\Users\\Opal\\anaconda3\\envs\\dashboard_ml_env\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    574\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[0;32m    576\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 577\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    579\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[0;32m    580\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\Opal\\anaconda3\\envs\\dashboard_ml_env\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1404\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m   1406\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1407\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[1;32mc:\\Users\\Opal\\anaconda3\\envs\\dashboard_ml_env\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1659\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[0;32m   1660\u001b[0m         mode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m-> 1661\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[0;32m   1662\u001b[0m     f,\n\u001b[0;32m   1663\u001b[0m     mode,\n\u001b[0;32m   1664\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1665\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1666\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[0;32m   1667\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[0;32m   1668\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   1669\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1670\u001b[0m )\n\u001b[0;32m   1671\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1672\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\Opal\\anaconda3\\envs\\dashboard_ml_env\\lib\\site-packages\\pandas\\io\\common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    854\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[0;32m    855\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    856\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    857\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[0;32m    858\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[1;32m--> 859\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[0;32m    860\u001b[0m             handle,\n\u001b[0;32m    861\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[0;32m    862\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[0;32m    863\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[0;32m    864\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    865\u001b[0m         )\n\u001b[0;32m    866\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    867\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[0;32m    868\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'bank.csv'"
     ]
    }
   ],
   "source": [
    "filepath = \"bank.csv\"\n",
    "col_to_drop = \"Unnamed: 0\"\n",
    "\n",
    "data = importData(path = filepath,\n",
    "                  col_to_drop = col_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Data Preprocessing\n",
    "## Train-Test Split\n",
    "​\n",
    "- Kita tidak pisahkan input-output, karena akan menganalisa struktur data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_test = train_test_split(data,\n",
    "                                         test_size = 0.25,\n",
    "                                         random_state = 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_train.shape)\n",
    "print(data_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical & Categorical Split\n",
    "- Cek unique value untuk setiap kolom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in data_train.columns:\n",
    "    print(f\"col: {col}, #unique: {len(data_train[col].unique())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Kita anggap `day` dan `month` sebagai numerik dalam latihan ini\n",
    "- karena jumlah unique valuenya besar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_col = [\"age\", \"day\", \"month\", \"balance\", \n",
    "           \"duration\", \"campaign\", \"pdays\", \"previous\"]\n",
    "cat_col = list(set(data_train.columns) - set(num_col))\n",
    "\n",
    "print(num_col)\n",
    "print(cat_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitNumCat(data, num_col, cat_col):\n",
    "    data_num = data[num_col]\n",
    "    data_cat = data[cat_col]\n",
    "\n",
    "    return data_num, data_cat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_num, data_train_cat = splitNumCat(data = data_train,\n",
    "                                             num_col = num_col,\n",
    "                                             cat_col = cat_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_num.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fitur month perlu di-transform jadi angka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_cat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Handling Data - Impute & Standardize\n",
    "**Transform** - fitur `month`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformMonth(data):\n",
    "    month_list = [\"jan\", \"feb\", \"mar\", \"apr\", \"may\", \"jun\",\n",
    "                  \"jul\", \"aug\", \"sep\", \"oct\", \"nov\", \"dec\"]\n",
    "    number_list = [i+1 for i in range(len(month_list))]\n",
    "\n",
    "    data[\"month\"] = data[\"month\"].replace(month_list, number_list)\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_num = transformMonth(data = data_train_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_num.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing Values - Numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cek missing value\n",
    "data_train_num.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buat imputer, kalau-kalau ada yang butuh di data test\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "def imputerNumeric(data, imputer = None):\n",
    "    if imputer == None:\n",
    "        # Buat imputer\n",
    "        imputer = SimpleImputer(missing_values = np.nan,\n",
    "                                strategy = \"median\")\n",
    "        imputer.fit(data)\n",
    "\n",
    "    # Transform data\n",
    "    data_imputed = imputer.transform(data)\n",
    "    data_imputed = pd.DataFrame(data = data_imputed,\n",
    "                                columns = data.columns,\n",
    "                                index = data.index)\n",
    "    \n",
    "    return data_imputed, imputer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_num_imputed, num_imputer = imputerNumeric(data = data_train_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_num_imputed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardizing - Numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Buat scaler\n",
    "def fitStandardize(data):\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(data)\n",
    "\n",
    "    return scaler\n",
    "\n",
    "# Transform scaler\n",
    "def transformStandardize(data, scaler):\n",
    "    data_scaled = scaler.transform(data)\n",
    "    data_scaled = pd.DataFrame(data = data_scaled,\n",
    "                               columns = data.columns,\n",
    "                               index = data.index)\n",
    "    \n",
    "    return data_scaled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cari scaler\n",
    "num_scaler = fitStandardize(data = data_train_num_imputed)\n",
    "\n",
    "# Transform data\n",
    "data_train_num_clean = transformStandardize(data = data_train_num_imputed,\n",
    "                                            scaler = num_scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_num_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Handling Data 2 - PCA (Dimensionality Reduction)\n",
    "\n",
    "**Goal**: represent data in fewer dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import package PCA - Sklearn\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define PCA with random state\n",
    "pca_obj = PCA(random_state = 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit to data_train_num_clean\n",
    "pca_obj.fit(data_train_num_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dapatkan principal component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show PCA Component\n",
    "pca_component = pca_obj.components_\n",
    "\n",
    "# Turn to dataframe\n",
    "pca_component = pd.DataFrame(data = pca_component,\n",
    "                             columns = data_train_num_clean.columns)\n",
    "pca_component"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dapatkan variance yang dijelaskan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explained variance\n",
    "pca_obj.explained_variance_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explained variance ratio\n",
    "pca_obj.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "bisa kita lihat,\n",
    "- PC 1 adalah baris pertama pada dataframe `pca_component`\n",
    "- PC 1 menjelaskan 19.3% variasi data\n",
    "*transform data dengan principal component*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform data\n",
    "data_train_num_pca = pca_obj.transform(data_train_num_clean)\n",
    "\n",
    "# Set data sebagai dataframe\n",
    "col_names = [f\"PC_{i+1}\" for i in range(data_train_num_pca.shape[1])]\n",
    "data_train_num_pca = pd.DataFrame(data = data_train_num_pca,\n",
    "                                  columns = col_names,\n",
    "                                  index = data_train_num_clean.index)\n",
    "\n",
    "data_train_num_pca.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Berapa principal component?*\n",
    "\n",
    "- Pilih untuk mempertahankan persentase variance tertentu dalam data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jika gunakan seluruh component, maka variance-nya\n",
    "sum(pca_obj.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jika memilih n component, maka variance yang dijelaskan\n",
    "for i in range(1, len(pca_obj.explained_variance_ratio_) + 1):\n",
    "    sum_of_variance_n = sum(pca_obj.explained_variance_ratio_[:i]) * 100\n",
    "    print(f\"n_component: {i}, %variance explained: {sum_of_variance_n:.2f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Apabila ingin mempertahankan 90% variance, maka Anda memilih 7 komponen\n",
    "- Jumlah komponen yang dipilih dapat dijadikan bagian dari eksperimentasi\n",
    "\n",
    "\n",
    "*Buat user-defined function untuk PCA*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitPCA(data):\n",
    "    # Buat objek PCA\n",
    "    pca_obj = PCA(random_state = 123)\n",
    "\n",
    "    # Fit PCA pada data\n",
    "    pca_obj.fit(data)\n",
    "\n",
    "    # Tampilkan explained-variance\n",
    "    print(\"Explained variance using n_components:\")\n",
    "    for i in range(1, len(pca_obj.explained_variance_ratio_) + 1):\n",
    "        sum_of_variance_n = sum(pca_obj.explained_variance_ratio_[:i]) * 100\n",
    "        print(f\"n_component: {i}, %variance explained: {sum_of_variance_n:.2f} %\")\n",
    "\n",
    "    print()\n",
    "\n",
    "    # Pilih n_components\n",
    "    n_comp = int(input(\"n_components : \"))\n",
    "\n",
    "    # Buat ulang PCA\n",
    "    pca_obj = PCA(n_components = n_comp,\n",
    "                  random_state = 123)\n",
    "    pca_obj.fit(data)\n",
    "\n",
    "    # Ekstrak komponen\n",
    "    pca_component = pca_obj.components_[:n_comp]\n",
    "\n",
    "    # Turn to dataframe\n",
    "    pca_component = pd.DataFrame(data = pca_component,\n",
    "                                columns = data.columns)\n",
    "    \n",
    "    return pca_component, pca_obj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_component, pca_obj = fitPCA(data = data_train_num_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buat fungsi transformasi data\n",
    "def transformPCA(data, pca_obj):\n",
    "    # Transform data\n",
    "    data_pca = pca_obj.transform(data)\n",
    "\n",
    "    cols = [f\"PC_{i+1}\" for i in range(data_pca.shape[1])]\n",
    "    data_pca = pd.DataFrame(data = data_pca,\n",
    "                            columns = cols,\n",
    "                            index = data.index)\n",
    "    \n",
    "    return data_pca\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_num_pca = transformPCA(data = data_train_num_clean,\n",
    "                                  pca_obj = pca_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cek data yang sudah diPCA\n",
    "data_train_num_pca.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cek komponen\n",
    "pca_component"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "membuat bi-plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_data = data_train_num_clean @ pca_component[:2].T\n",
    "transformed_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows = 1, ncols = 1, figsize = (10, 7))\n",
    "\n",
    "ax.scatter(transformed_data[0][data_train_cat[\"poutcome\"]==\"success\"], \n",
    "           transformed_data[1][data_train_cat[\"poutcome\"]==\"success\"], \n",
    "           marker=\".\", \n",
    "           c=\"red\", #s=10,\n",
    "           alpha=.2,\n",
    "           label = \"SUCCESS\")\n",
    "\n",
    "ax.scatter(transformed_data[0][data_train_cat[\"poutcome\"]==\"other\"], \n",
    "           transformed_data[1][data_train_cat[\"poutcome\"]==\"other\"], \n",
    "           marker=\".\", \n",
    "           c=\"blue\", #s=10,\n",
    "           alpha=.2,\n",
    "           label = \"FAILED\")\n",
    "\n",
    "for col in pca_component.columns:\n",
    "    data_col = np.array(pca_component[col].loc[0:1])*5.\n",
    "    start_point = [0, data_col[0]]\n",
    "    end_point = [0, data_col[1]]\n",
    "\n",
    "    ax.plot(start_point, end_point, marker=\"o\", label=col)\n",
    "\n",
    "ax.set_ylabel(\"Second Principal Components\")\n",
    "ax.set_xlabel(\"First Principal Components\")\n",
    "ax.set_xlim([-2.0, 5])\n",
    "ax.set_ylim([-2.0, 5])\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gimana cara interpretasinya?\n",
    "- Untuk PC_1, memberi bobot besar pada `pdays` dan `previous`, tapi bobot untuk `duration`, `balance` dan `age` kecil\n",
    "- Artinya `pdays` dan `previous` berkorelasi satu sama lain,\n",
    "- Semakin besar `previous`, semakin besar `pdays`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "# Modeling Clustering - Data Full\n",
    "- **Goal**: make separate group with similar character, and assign them into cluster\n",
    "- **TASK CLUSTERING IS SUBJECTIVE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "buat objek clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buat objek k-means\n",
    "kmeans_obj = KMeans(n_clusters = 3,\n",
    "                    random_state = 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit objek k-means\n",
    "kmeans_obj.fit(data_train_num_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "predict clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict Cluster\n",
    "kmeans_obj.predict(data_train_num_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape predicted cluster to dataframe\n",
    "cluster_result = kmeans_obj.predict(data_train_num_clean)\n",
    "cluster_result = pd.DataFrame(data = cluster_result,\n",
    "                              columns = [\"cluster\"],\n",
    "                              index = data_train_num_clean.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_result.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "periksa proporsi cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_result[\"cluster\"].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 2 cluster memiliki porsi di atas 43% data\n",
    "\n",
    "periksa centroid sebagai representasi cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check centroid\n",
    "kmeans_obj.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jadikan dataframe\n",
    "centroids = kmeans_obj.cluster_centers_\n",
    "centroids = pd.DataFrame(data = centroids,\n",
    "                         columns = data_train_num_clean.columns)\n",
    "\n",
    "centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tentu hal diatas tidak bisa diartikan\n",
    "- Karena dalam bentuk terstandardkan\n",
    "- Kita harus balikan ke dalam bentuk awal sebelum distandarisasi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inverse transform dari standardizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroid_real = num_scaler.inverse_transform(centroids)\n",
    "centroid_real = pd.DataFrame(data = centroid_real,\n",
    "                             columns = data_train_num_clean.columns)\n",
    "\n",
    "centroid_real"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "*lalu artinya apa?* - Harus di translate sendiri\n",
    "- Cluster 1 (0) adalah **group** yang\n",
    "    - dikontak di awal bulan\n",
    "    - sudah pernah dikontak 2x **selama** campaign\n",
    "    - belum pernah dikontak **sebelum** campaign\n",
    "    \n",
    "*BEST K?*\n",
    "\n",
    "Score -- within-cluster sum-of-squares\n",
    "​\n",
    "$$\n",
    "\\text{scores} = - \\sum_{i=0}^{n} ||x_{i} - \\mu_{j}||^{2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tampilkan score\n",
    "-kmeans_obj.score(data_train_num_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "coba variasikan beberapa cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_list = []\n",
    "k_list = np.arange(2, 11, 1)\n",
    "\n",
    "for k in k_list:\n",
    "    # Buat object\n",
    "    kmeans_obj_k = KMeans(n_clusters = k,\n",
    "                          max_iter = 50,\n",
    "                          random_state = 123)\n",
    "    \n",
    "    # Fit data\n",
    "    kmeans_obj_k.fit(data_train_num_clean)\n",
    "\n",
    "    # update score\n",
    "    score_k = -kmeans_obj_k.score(data_train_num_clean)\n",
    "    score_list.append(score_k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(10, 7))\n",
    "\n",
    "ax.plot(k_list, score_list, \"r\", marker=\"o\")\n",
    "\n",
    "ax.set_xlabel(\"number of cluster\")\n",
    "ax.set_ylabel(\"within-cluster sum-of-square\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Makin banyak cluster, makin rendah scorenya.\n",
    "- Tapi, makin banyak cluster, makin kompleks untuk diinterpretasikan.\n",
    "- Kita coba ambil cluster terbaik di 9, karena perubahan error di cluster 10 mengecil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buat object\n",
    "kmeans_obj_best = KMeans(n_clusters = 9,\n",
    "                         random_state = 123)\n",
    "\n",
    "# Fit object\n",
    "kmeans_obj_best.fit(data_train_num_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tampilkan Centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jadikan centroid dalam bentuk dataframe\n",
    "centroids_best = kmeans_obj_best.cluster_centers_\n",
    "centroids_best = pd.DataFrame(data = centroids_best,\n",
    "                              columns = data_train_num_clean.columns)\n",
    "\n",
    "# Inverse transform\n",
    "centroid_real_best = num_scaler.inverse_transform(centroids_best)\n",
    "centroid_real_best = pd.DataFrame(data = centroid_real_best,\n",
    "                                  columns = data_train_num_clean.columns)\n",
    "\n",
    "centroid_real_best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "*Coba Interpretasikan di atas ini?*\n",
    "\n",
    "\n",
    "**Predict Cluster**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_best = kmeans_obj_best.predict(data_train_num_clean)\n",
    "\n",
    "cluster_best = pd.DataFrame(data = cluster_best,\n",
    "                            columns = [\"cluster\"],\n",
    "                            index= data_train_num_clean.index)\n",
    "cluster_best.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "# Modeling Clustering - Data PCA\n",
    "\n",
    "\n",
    "*Variasikan beberapa cluster*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_list = []\n",
    "k_list = np.arange(2, 11, 1)\n",
    "\n",
    "for k in k_list:\n",
    "    # Buat object\n",
    "    kmeans_obj_k = KMeans(n_clusters = k,\n",
    "                          max_iter = 50,\n",
    "                          random_state = 123)\n",
    "    \n",
    "    # Fit data\n",
    "    kmeans_obj_k.fit(data_train_num_pca)\n",
    "\n",
    "    # update score\n",
    "    score_k = -kmeans_obj_k.score(data_train_num_pca)\n",
    "    score_list.append(score_k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(10, 7))\n",
    "\n",
    "ax.plot(k_list, score_list, \"r\", marker=\"o\")\n",
    "\n",
    "ax.set_xlabel(\"number of cluster\")\n",
    "ax.set_ylabel(\"within-cluster sum-of-square\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Makin banyak cluster, makin rendah scorenya.\n",
    "- Tapi, makin banyak cluster, makin kompleks untuk diinterpretasikan.\n",
    "- Kita coba ambil cluster terbaik di 7, karena perubahan error di cluster selanjutnya mengecil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buat object\n",
    "kmeans_obj_pca_best = KMeans(n_clusters = 7,\n",
    "                             random_state = 123)\n",
    "\n",
    "# Fit object\n",
    "kmeans_obj_pca_best.fit(data_train_num_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_pca_best = kmeans_obj_pca_best.predict(data_train_num_pca)\n",
    "\n",
    "cluster_pca_best = pd.DataFrame(data = cluster_pca_best,\n",
    "                                columns = [\"cluster\"],\n",
    "                                index = data_train_num_pca.index)\n",
    "cluster_pca_best.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Centroid PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cari centroid\n",
    "centroid_pca_best = kmeans_obj_pca_best.cluster_centers_\n",
    "centroid_pca_best = pd.DataFrame(data = centroid_pca_best,\n",
    "                                 columns = data_train_num_pca.columns)\n",
    "centroid_pca_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inverse transform centroid pca\n",
    "# agar dapat diinterpretasikan\n",
    "centroid_pca_best_inv = pca_obj.inverse_transform(centroid_pca_best)\n",
    "centroid_pca_best_inv = pd.DataFrame(centroid_pca_best_inv,\n",
    "                                     columns = data_train_num_clean.columns)\n",
    "centroid_pca_best_inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inverse transform centroid standardisasi\n",
    "# agar dapat diinterpretasikan\n",
    "centroid_pca_best_real = num_scaler.inverse_transform(centroid_pca_best_inv)\n",
    "centroid_pca_best_real = pd.DataFrame(centroid_pca_best_real,\n",
    "                                      columns = data_train_num_clean.columns)\n",
    "\n",
    "centroid_pca_best_real"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sekarang, data bisa diinterpretasikan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Clustering Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformTestData(data, num_col, cat_col, num_imputer, num_scaler):\n",
    "    # 1. Split num-cat data\n",
    "    data_num, _ = splitNumCat(data = data,\n",
    "                              num_col = num_col,\n",
    "                              cat_col = cat_col)\n",
    "    \n",
    "    # 2. Handling Data\n",
    "    # 2.1 transform month\n",
    "    data_num = transformMonth(data = data_num)\n",
    "\n",
    "    # 2.2 impute data\n",
    "    data_num_imputed, _= imputerNumeric(data = data_train_num,\n",
    "                                        imputer = num_imputer)\n",
    "    \n",
    "    # 2.3 Standardization\n",
    "    data_num_scaled = transformStandardize(data = data_num_imputed,\n",
    "                                           scaler = num_scaler)\n",
    "    \n",
    "    return data_num_scaled\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test_clean = transformTestData(data = data_test,\n",
    "                                    num_col = num_col,\n",
    "                                    cat_col = cat_col,\n",
    "                                    num_imputer = num_imputer,\n",
    "                                    num_scaler = num_scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test_clean_pca = transformPCA(data = data_test_clean,\n",
    "                                   pca_obj = pca_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test_clean_pca.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "predict data test - FULL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_obj_best.predict(data_test_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_obj_pca_best.predict(data_test_clean_pca)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dashboard_ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
